{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUcjPoknmqpeaX+mcqYCxw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "90e43c59afc140b5810396814412e57b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a387f8263bbe4347a7a54903f5de6f84",
              "IPY_MODEL_6d4d8b6977d44d5b9808b343059ad9fc",
              "IPY_MODEL_f062bd2db2f7406ea301d44b4e0d56a5"
            ],
            "layout": "IPY_MODEL_fae3beca871646d4a3b158835411c641"
          }
        },
        "a387f8263bbe4347a7a54903f5de6f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f98a05328d4c445c874e382828f53949",
            "placeholder": "​",
            "style": "IPY_MODEL_66d7ec2c4f9d487fafb80e7d7543b4c1",
            "value": "Summarizing chunks: 100%"
          }
        },
        "6d4d8b6977d44d5b9808b343059ad9fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f0a8847e088432aa8b67ad58349ae8a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7043381b4c264108bd763cb213ad9f2e",
            "value": 1
          }
        },
        "f062bd2db2f7406ea301d44b4e0d56a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab037aba705948c5b9f180159f32bd4e",
            "placeholder": "​",
            "style": "IPY_MODEL_26165408aa7a499883045aa4da89ae38",
            "value": " 1/1 [00:33&lt;00:00, 33.62s/it]"
          }
        },
        "fae3beca871646d4a3b158835411c641": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f98a05328d4c445c874e382828f53949": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66d7ec2c4f9d487fafb80e7d7543b4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f0a8847e088432aa8b67ad58349ae8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7043381b4c264108bd763cb213ad9f2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab037aba705948c5b9f180159f32bd4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26165408aa7a499883045aa4da89ae38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LohithVarun/NLP_Project/blob/main/NLP_Text_Summarizer_Real.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --no-deps xformers trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "-fTmgBGfJsNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install PyPDF2\n",
        "!pip install rouge\n",
        "!pip install sentencepiece\n",
        "!pip install unsloth_zoo\n",
        "!pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
      ],
      "metadata": {
        "id": "mmMSGIp1Pv3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from typing import List, Dict, Optional\n",
        "import logging\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "class ResearchPaperSummarizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "        max_seq_length: int = 2048*6,\n",
        "        load_in_4bit: bool = True\n",
        "    ):\n",
        "        \"\"\"Initialize the research paper summarizer with Llama 3.2 model\"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.logger = self._setup_logger()\n",
        "\n",
        "        # Initialize model using unsloth\n",
        "        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=model_name,\n",
        "            max_seq_length=max_seq_length,\n",
        "            dtype=None,  # Auto detect\n",
        "            load_in_4bit=load_in_4bit,\n",
        "        )\n",
        "\n",
        "        # Configure model with LoRA for efficient fine-tuning\n",
        "        self.model = FastLanguageModel.get_peft_model(\n",
        "            self.model,\n",
        "            r=16,\n",
        "            target_modules=[\n",
        "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "            ],\n",
        "            lora_alpha=16,\n",
        "            lora_dropout=0,\n",
        "            bias=\"none\",\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "            random_state=3407,\n",
        "        )\n",
        "\n",
        "        # Set up for inference\n",
        "        FastLanguageModel.for_inference(self.model)\n",
        "\n",
        "        # Set up chat template\n",
        "        self.tokenizer = get_chat_template(\n",
        "            self.tokenizer,\n",
        "            chat_template=\"llama-3.1\",\n",
        "        )\n",
        "\n",
        "    def _setup_logger(self) -> logging.Logger:\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        return logging.getLogger(__name__)\n",
        "\n",
        "    def read_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF file\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text()\n",
        "                return self._preprocess_text(text)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error reading PDF file: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def _preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and preprocess the input text\"\"\"\n",
        "        # Remove extra whitespace and normalize\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # Remove special characters but keep essential punctuation\n",
        "        text = re.sub(r'[^\\w\\s.,!?;:()\\-\\']', '', text)\n",
        "        # Clean up any double spaces created\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _chunk_text(self, text: str, max_chunk_size: int = 2048*5) -> List[str]:\n",
        "        \"\"\"Split text into manageable chunks for processing\"\"\"\n",
        "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_length = len(sentence.split())\n",
        "            if current_length + sentence_length <= max_chunk_size:\n",
        "                current_chunk.append(sentence)\n",
        "                current_length += sentence_length\n",
        "            else:\n",
        "                if current_chunk:\n",
        "                    chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_length = sentence_length\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _generate_summary(self, text: str, max_new_tokens: int = 500) -> str:\n",
        "        \"\"\"Generate summary for a chunk of text\"\"\"\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Please summarize the following research paper excerpt. Focus on key findings, \"\n",
        "                \"methodology, and conclusions. Format the summary in clear, concise language:\\n\\n\"\n",
        "                f\"{text}\"\n",
        "            )\n",
        "        }]\n",
        "\n",
        "        inputs = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(self.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=0.7,\n",
        "            min_p=0.1,\n",
        "            do_sample=True,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the assistant's response\n",
        "        summary = summary.split(\"assistant\")[1].split(\"<|eot\")[0].strip()\n",
        "        return summary\n",
        "\n",
        "    def summarize(self, file_path: str) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate a comprehensive summary of a research paper\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the PDF or text file containing the research paper\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing the summary and metadata\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Read and preprocess the paper\n",
        "            if file_path.endswith('.pdf'):\n",
        "                text = self.read_pdf(file_path)\n",
        "            else:\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = self._preprocess_text(file.read())\n",
        "\n",
        "            if not text:\n",
        "                raise ValueError(\"No text could be extracted from the file\")\n",
        "\n",
        "            # Split into chunks and summarize each section\n",
        "            chunks = self._chunk_text(text)\n",
        "            self.logger.info(f\"Processing paper in {len(chunks)} chunks\")\n",
        "\n",
        "            summaries = []\n",
        "            for chunk in tqdm(chunks, desc=\"Summarizing chunks\"):\n",
        "                summary = self._generate_summary(chunk)\n",
        "                summaries.append(summary)\n",
        "\n",
        "            # Combine chunk summaries into a final summary\n",
        "            combined_summary = \" \".join(summaries)\n",
        "\n",
        "            # Generate a final, condensed summary\n",
        "            final_summary = self._generate_summary(\n",
        "                \"Please provide a concise, well-structured final summary of this research paper: \" +\n",
        "                combined_summary\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"summary\": final_summary,\n",
        "                \"original_length\": len(text.split()),\n",
        "                \"summary_length\": len(final_summary.split()),\n",
        "                \"num_chunks_processed\": len(chunks)\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in summarization process: {str(e)}\")\n",
        "            return {\n",
        "                \"summary\": \"\",\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    summarizer = ResearchPaperSummarizer()\n",
        "\n",
        "    # Example with a research paper\n",
        "    result = summarizer.summarize(\"test_nlp.pdf\")\n",
        "\n",
        "    if \"error\" in result:\n",
        "        print(f\"Error: {result['error']}\")\n",
        "    else:\n",
        "        print(\"\\nSummary:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(result[\"summary\"])\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"\\nOriginal length: {result['original_length']} words\")\n",
        "        print(f\"Summary length: {result['summary_length']} words\")\n",
        "        print(f\"Compression ratio: {result['summary_length']/result['original_length']:.2%}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2MtTWn0uF-uV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "90e43c59afc140b5810396814412e57b",
            "a387f8263bbe4347a7a54903f5de6f84",
            "6d4d8b6977d44d5b9808b343059ad9fc",
            "f062bd2db2f7406ea301d44b4e0d56a5",
            "fae3beca871646d4a3b158835411c641",
            "f98a05328d4c445c874e382828f53949",
            "66d7ec2c4f9d487fafb80e7d7543b4c1",
            "0f0a8847e088432aa8b67ad58349ae8a",
            "7043381b4c264108bd763cb213ad9f2e",
            "ab037aba705948c5b9f180159f32bd4e",
            "26165408aa7a499883045aa4da89ae38"
          ]
        },
        "outputId": "5aa1029f-1ed1-4021-a810-f40760028787"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.2.12: Fast Llama patching. Transformers: 4.50.0.dev0.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Summarizing chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90e43c59afc140b5810396814412e57b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary:\n",
            "--------------------------------------------------------------------------------\n",
            "**Summary of Research Paper Excerpt: Sentiment Analysis**\n",
            "\n",
            "**Key Findings:**\n",
            "\n",
            "1. Sentiment analysis is crucial in various applications, including customer service, market research, and social media monitoring.\n",
            "2. The complexity of language and variability of human emotions pose significant challenges for sentiment analysis.\n",
            "3. Machine learning algorithms and NLP techniques are effective approaches to analyzing sentiment in text data.\n",
            "4. Human evaluation is critical in sentiment analysis, and more effective methods are needed to evaluate sentiment analysis systems.\n",
            "\n",
            "**Methodology:**\n",
            "\n",
            "1. The authors reviewed various approaches to sentiment analysis, including machine learning algorithms, NLP techniques, and rule-based systems.\n",
            "2. They evaluated the strengths and weaknesses of each approach and provided examples of their application in different domains.\n",
            "3. A new approach to sentiment analysis was presented, combining machine learning algorithms and NLP techniques.\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "1. Sentiment analysis is a critical component of various applications, and its importance cannot be overstated.\n",
            "2. The complexity of language and variability of human emotions require effective approaches to sentiment analysis.\n",
            "3. The need for human evaluation in sentiment analysis is emphasized, and more effective methods are necessary to evaluate sentiment analysis systems.\n",
            "\n",
            "Overall, the research paper provides a comprehensive overview of sentiment analysis, highlighting its importance and challenges in various applications.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Original length: 3820 words\n",
            "Summary length: 205 words\n",
            "Compression ratio: 5.37%\n"
          ]
        }
      ]
    }
  ]
}