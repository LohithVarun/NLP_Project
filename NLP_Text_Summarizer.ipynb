{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGjklu2LlEvpRa9ZaSHXlI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LohithVarun/NLP_Project/blob/main/NLP_Text_Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install transformers\n",
        "!pip install PyPDF2\n",
        "!pip install rouge\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install sentencepiece\n",
        "\n",
        "import torch\n",
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from transformers import pipeline\n",
        "import PyPDF2\n",
        "import os\n",
        "import numpy as np\n",
        "from rouge import Rouge\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import logging\n",
        "\n",
        "class TextSummarizer:\n",
        "    def __init__(self, model_name: str = \"google/pegasus-large\"):\n",
        "        \"\"\"\n",
        "        Initialize the summarizer with specified model\n",
        "        Args:\n",
        "            model_name (str): Name of the pretrained model to use\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "        self.model = PegasusForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
        "        self.rouge = Rouge()\n",
        "        self.logger = self._setup_logger()\n",
        "\n",
        "    def _setup_logger(self) -> logging.Logger:\n",
        "        \"\"\"Setup logging configuration\"\"\"\n",
        "        logging.basicConfig(level=logging.INFO)\n",
        "        return logging.getLogger(__name__)\n",
        "\n",
        "    def read_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Read and extract text from PDF file\n",
        "        Args:\n",
        "            file_path (str): Path to the PDF file\n",
        "        Returns:\n",
        "            str: Extracted text from PDF\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text()\n",
        "                return self._preprocess_text(text)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error reading PDF file: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def _preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Preprocess the input text\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "        Returns:\n",
        "            str: Preprocessed text\n",
        "        \"\"\"\n",
        "        # Remove extra whitespace and special characters\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def _chunk_text(self, text: str, max_chunk_size: int = 1024) -> List[str]:\n",
        "        \"\"\"\n",
        "        Split text into chunks for processing\n",
        "        Args:\n",
        "            text (str): Input text\n",
        "            max_chunk_size (int): Maximum chunk size\n",
        "        Returns:\n",
        "            List[str]: List of text chunks\n",
        "        \"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_size = 0\n",
        "\n",
        "        for word in words:\n",
        "            if current_size + len(word) + 1 <= max_chunk_size:\n",
        "                current_chunk.append(word)\n",
        "                current_size += len(word) + 1\n",
        "            else:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [word]\n",
        "                current_size = len(word) + 1\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def summarize(self, file_path: str, max_length: int = 150, min_length: int = 50) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate summary for the given file\n",
        "        Args:\n",
        "            file_path (str): Path to the input file\n",
        "            max_length (int): Maximum length of the summary\n",
        "            min_length (int): Minimum length of the summary\n",
        "        Returns:\n",
        "            Dict: Dictionary containing summary and metrics\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Read and preprocess text\n",
        "            if file_path.endswith('.pdf'):\n",
        "                text = self.read_pdf(file_path)\n",
        "            else:\n",
        "                with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                    text = self._preprocess_text(file.read())\n",
        "\n",
        "            if not text:\n",
        "                raise ValueError(\"No text could be extracted from the file\")\n",
        "\n",
        "            # Split text into chunks\n",
        "            chunks = self._chunk_text(text)\n",
        "            summaries = []\n",
        "\n",
        "            # Process each chunk\n",
        "            for chunk in chunks:\n",
        "                inputs = self.tokenizer(chunk, return_tensors=\"pt\", max_length=1024, truncation=True).to(self.device)\n",
        "                summary_ids = self.model.generate(\n",
        "                    inputs[\"input_ids\"],\n",
        "                    max_length=max_length,\n",
        "                    min_length=min_length,\n",
        "                    num_beams=4,\n",
        "                    length_penalty=2.0,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "                summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "                summaries.append(summary)\n",
        "\n",
        "            # Combine summaries\n",
        "            final_summary = \" \".join(summaries)\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            if len(text.split()) > 10:  # Only calculate ROUGE if there's enough text\n",
        "                scores = self.rouge.get_scores(final_summary, text)\n",
        "            else:\n",
        "                scores = None\n",
        "\n",
        "            return {\n",
        "                \"summary\": final_summary,\n",
        "                \"rouge_scores\": scores,\n",
        "                \"original_length\": len(text.split()),\n",
        "                \"summary_length\": len(final_summary.split())\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in summarization: {str(e)}\")\n",
        "            return {\n",
        "                \"summary\": \"\",\n",
        "                \"error\": str(e)\n",
        "            }\n",
        "\n",
        "# Example usage\n",
        "def main():\n",
        "    summarizer = TextSummarizer()\n",
        "\n",
        "    # Example with sample file\n",
        "    result = summarizer.summarize(\"NLP_Text_Summarization.pdf\")\n",
        "\n",
        "    print(\"Summary:\", result[\"summary\"])\n",
        "    if result.get(\"rouge_scores\"):\n",
        "        print(\"\\nROUGE Scores:\")\n",
        "        print(\"ROUGE-1:\", result[\"rouge_scores\"][0][\"rouge-1\"])\n",
        "        print(\"ROUGE-2:\", result[\"rouge_scores\"][0][\"rouge-2\"])\n",
        "        print(\"ROUGE-L:\", result[\"rouge_scores\"][0][\"rouge-L\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2MtTWn0uF-uV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}